# DO NOT EDIT â€” this file is generated from cm.yaml and will be overwritten.
# To make changes, edit cm.yaml and run: cm update
# Generated by container-magic: https://github.com/markhedleyjones/container-magic
# Config hash: {{ config_hash }}

# Container runtime configuration
RUNTIME := "{{ runtime }}"
IMAGE_NAME := "{{ project_name }}"
WORKSPACE_NAME := "{{ workspace_name }}"
USER_CWD := ""

# Check if configuration has changed and warn user
_check-config force="":
    #!/usr/bin/env bash
    expected="{{ config_hash }}"

    # Find config file (prefer cm.yaml over container-magic.yaml)
    if [ -f "cm.yaml" ]; then
        config_file="cm.yaml"
    elif [ -f "container-magic.yaml" ]; then
        config_file="container-magic.yaml"
    else
        echo "âš ï¸  No config file found (cm.yaml or container-magic.yaml)"
        exit 1
    fi

    current=$(sha256sum "$config_file" | cut -d' ' -f1)
    if [ "$current" != "$expected" ]; then
        # Check if auto_update is disabled in the config file (read at runtime)
        auto_update_off=$(grep -E '^\s*auto_update:\s*false' "$config_file" || echo "")

        if [ -n "$auto_update_off" ]; then
            if [ "{{ '{{force}}' }}" != "--force" ]; then
                echo "âš ï¸  $config_file has changed since last generation"
                echo "   Run 'cm update' to regenerate Justfile and Dockerfile"
                echo "   Or use --force to build anyway"
                echo ""
                exit 1
            fi
        else
            # Auto-update enabled (default) - regenerate files
            echo "ðŸ“ Config changed, regenerating files..."
            cm update
        fi
    fi

# Build development image
build force="": (_check-config force)
    {% if use_host_user %}
    {{runtime}} build \
        --target {{ dev_stage }} \
        --build-arg USER_GID=$(id --group) \
        --build-arg USER_UID=$(id --user) \
        --build-arg USER_NAME=$(id --user --name) \
        --build-arg USER_HOME=$(echo ~) \
        --build-arg WORKDIR=$(echo ~) \
        --tag {{ project_name }}:development \
        .
    {% else %}
    {{runtime}} build \
        --target {{ dev_stage }} \
        --build-arg USER_GID={{ dev_user_gid }} \
        --build-arg USER_UID={{ dev_user_uid }} \
        --build-arg USER_NAME={{ dev_user_name }} \
        --build-arg USER_HOME={{ dev_user_home }} \
        --build-arg WORKDIR={{ dev_user_home }} \
        --tag {{ project_name }}:development \
        .
    {% endif %}

# Build production image (if production stage exists)
build-production: _check-config
    {{runtime}} build \
        --target production \
        --tag {{ project_name }}:latest \
        .

# Run command in container
run *args:
    #!/usr/bin/env bash
    set -euo pipefail

    # Parse --detach/-d flag (first argument only)
    DETACH=false
    set +u
    COMMAND="{{ '{{args}}' }}"
    set -u
    FIRST_WORD="${COMMAND%% *}"
    if [[ "${FIRST_WORD}" == "--detach" ]] || [[ "${FIRST_WORD}" == "-d" ]]; then
        DETACH=true
        if [[ "${COMMAND}" == "${FIRST_WORD}" ]]; then
            COMMAND=""
        else
            COMMAND="${COMMAND#* }"
        fi
    fi

    # Container configuration
    CONTAINER_NAME="{{ project_name }}-development"
    IMAGE="{{ project_name }}:development"

    # Build run arguments
    RUN_ARGS=()
    RUN_ARGS+=("{{ runtime }}" "run")
    RUN_ARGS+=("--name" "${CONTAINER_NAME}")
    RUN_ARGS+=("--rm")
    {% if ipc %}
    RUN_ARGS+=("--ipc" "{{ ipc }}")
    {% endif %}
    {% if network %}
    RUN_ARGS+=("--net" "{{ network }}")
    {% endif %}

    {% if runtime == "podman" %}
    # Podman-specific configuration
    RUN_ARGS+=("--replace")
    RUN_ARGS+=("--userns=keep-id")
    RUN_ARGS+=("--env-host=false")
    {% endif %}
    {% if privileged %}

    # Privileged mode
    RUN_ARGS+=("--privileged")
    {% endif %}

    {% if mount_workspace %}
    # Mount workspace
    RUN_ARGS+=("-v" "$(pwd)/{{ workspace_name }}:{{ container_home }}/{{ workspace_name }}:z")
    {% endif %}
    {% if volumes %}

    # Additional bind mounts
    {% for volume in volumes %}
    RUN_ARGS+=("-v" "{{ volume }}")
    {% endfor %}
    {% endif %}
    {% if devices %}

    # Device passthrough
    {% for device in devices %}
    RUN_ARGS+=("--device" "{{ device }}")
    {% endfor %}
    {% endif %}
    {% if features.display %}

    # Display support (Wayland and X11)
    {% if runtime == "podman" %}
    RUN_ARGS+=("--security-opt=label=disable")
    {% endif %}
    XHOST_CLEANUP=false
    if [[ -n "${WAYLAND_DISPLAY:-}" ]]; then
        wayland_socket="${XDG_RUNTIME_DIR}/${WAYLAND_DISPLAY}"
        RUN_ARGS+=("-e" "WAYLAND_DISPLAY")
        RUN_ARGS+=("-e" "XDG_RUNTIME_DIR=${XDG_RUNTIME_DIR}")
        RUN_ARGS+=("-v" "${wayland_socket}:${wayland_socket}")
    fi
    if [[ -n "${DISPLAY:-}" ]]; then
        xsock=/tmp/.X11-unix
    {% if runtime == "podman" %}
        xauth=/tmp/.docker.xauth
        touch "$xauth"
        if [[ -f "$HOME/.Xauthority" ]]; then
            xauth nlist "${DISPLAY}" | sed -e 's/^..../ffff/' | xauth -f "${xauth}" nmerge -
        fi
        RUN_ARGS+=("-e" "XAUTHORITY=${xauth}")
        RUN_ARGS+=("-v" "${xauth}:${xauth}")
    {% else %}
        xhost +local: >/dev/null 2>&1
        XHOST_CLEANUP=true
    {% endif %}
        RUN_ARGS+=("-e" "DISPLAY")
        RUN_ARGS+=("-v" "${xsock}:${xsock}")
        RUN_ARGS+=("--env" "QT_X11_NO_MITSHM=1")
    fi
    if [[ "${XHOST_CLEANUP}" == true ]] && [[ "${DETACH}" != true ]]; then
        trap 'xhost -local: >/dev/null 2>&1' EXIT
    fi
    {% endif %}
    {% if features.gpu %}

    # GPU support
    if [[ -d /dev/dri ]]; then
        RUN_ARGS+=("--device" "/dev/dri:/dev/dri")
    fi
    if command -v nvidia-smi >/dev/null 2>&1; then
        RUN_ARGS+=("-e" "NVIDIA_DRIVER_CAPABILITIES=all")
        {% if runtime == "docker" %}
        RUN_ARGS+=("--gpus=all")
        {% elif runtime == "podman" %}
        RUN_ARGS+=("--device" "nvidia.com/gpu=all")
        RUN_ARGS+=("--annotation=run.oci.keep_original_groups=1")
        RUN_ARGS+=("--security-opt=label=disable")
        {% endif %}
    fi
    {% endif %}
    {% if features.audio %}

    # Audio support (PulseAudio/PipeWire)
    if [[ -S "/run/user/$(id -u)/pulse/native" ]]; then
        RUN_ARGS+=("-v" "/run/user/$(id -u)/pulse/native:/run/user/$(id -u)/pulse/native")
        RUN_ARGS+=("-e" "PULSE_SERVER=unix:/run/user/$(id -u)/pulse/native")
    fi
    {% endif %}
    {% if features.aws_credentials %}

    # AWS credentials
    if [[ -d "$HOME/.aws" ]]; then
        RUN_ARGS+=("-v" "$HOME/.aws:{{ container_home }}/.aws:z")
    fi
    {% endif %}

    # Working directory - translate user's cwd to container path
    if [[ -n "{{ '{{USER_CWD}}' }}" ]]; then
        # Calculate relative path from project root to user's cwd
        PROJECT_ROOT="$(pwd)"
        USER_PATH="{{ '{{USER_CWD}}' }}"

        # Get relative path
        REL_PATH=$(realpath --relative-to="$PROJECT_ROOT" "$USER_PATH" 2>/dev/null || echo "")

        if [[ -n "$REL_PATH" && "$REL_PATH" != "." ]]; then
            # User is in a subdirectory - set container workdir accordingly
            RUN_ARGS+=("--workdir={{ container_home }}/${REL_PATH}")
        else
            # User is at project root
            RUN_ARGS+=("--workdir={{ container_home }}")
        fi
    else
        # No USER_CWD set (called via 'just' directly) - use project root
        RUN_ARGS+=("--workdir={{ container_home }}")
    fi

    if [[ "${DETACH}" == true ]]; then
        RUN_ARGS+=("--detach")
        RUN_ARGS+=("${IMAGE}")
        if [[ -n "${COMMAND}" ]]; then
            RUN_ARGS+=("{{ shell }}" "-c" "${COMMAND}")
        else
            RUN_ARGS+=("{{ shell }}")
        fi
        "${RUN_ARGS[@]}"
    else
        if [[ -z "${COMMAND}" ]] && [[ -t 0 ]]; then
            RUN_ARGS+=("--interactive" "--tty")
        fi

        RUN_ARGS+=("${IMAGE}")

        if [[ -n "${COMMAND}" ]]; then
            RUN_ARGS+=("{{ shell }}" "-c" "${COMMAND}")
        else
            RUN_ARGS+=("{{ shell }}")
        fi

        # Check if container already running
        if ! {{ runtime }} ps --quiet --filter name="^${CONTAINER_NAME}$" | grep -q .; then
            "${RUN_ARGS[@]}"
        else
            # Use exec instead for running container
            EXEC_ARGS=("{{ runtime }}" "exec")
            EXEC_ARGS+=("${CONTAINER_NAME}")
            if [[ -n "${COMMAND}" ]]; then
                EXEC_ARGS+=("{{ shell }}" "-c" "${COMMAND}")
            else
                # Interactive mode: add TTY flags only when running interactive shell
                if [[ -t 0 ]]; then
                    # Insert TTY flags before container name
                    EXEC_ARGS=("{{ runtime }}" "exec" "--interactive" "--tty" "${CONTAINER_NAME}")
                fi
                EXEC_ARGS+=("{{ shell }}")
            fi
            "${EXEC_ARGS[@]}"
        fi
    fi

# Open interactive shell
shell: _check-config
    @just run

# Stop development container
stop:
    #!/usr/bin/env bash
    {{ runtime }} stop {{ project_name }}-development 2>/dev/null || true
    {% if features.display and runtime == "docker" %}
    xhost -local: >/dev/null 2>&1 || true
    {% endif %}

# Stop and remove containers
clean:
    -{{ runtime }} stop {{ project_name }}-development 2>/dev/null
    -{{ runtime }} rm {{ project_name }}-development 2>/dev/null

# Remove images
clean-images:
    -{{ runtime }} rmi {{ project_name }}:development 2>/dev/null
    -{{ runtime }} rmi {{ project_name }}:latest 2>/dev/null
